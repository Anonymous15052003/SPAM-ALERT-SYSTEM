# -*- coding: utf-8 -*-
"""KAVACH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WaHoGMEctsxWpXF1mophdMM3Ngrlhdj4
"""

import pandas as pd

from sklearn.model_selection import train_test_split

from sklearn.feature_extraction.text import CountVectorizer

from sklearn import svm

import numpy as p

df=pd.read_csv('/content/spam.csv',encoding='latin-1')

df.head()

df.info()

df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)

df.sample(5)

df.rename(columns={'v1':'message_type','v2':'message'},inplace=True)

df.sample(5)

from sklearn.preprocessing import LabelEncoder

encoder=LabelEncoder()

df['message_type']=encoder.fit_transform(df['message_type'])

df['message_type'].sample(5)

df[df['message_type']==1]

df.isnull().sum()

df.duplicated().sum()

import seaborn as sns

import matplotlib.pyplot as plt

df['message_type'].value_counts()

plt.pie(df['message_type'].value_counts(),labels=['not spam','spam'],autopct='%0.2f')
plt.show()

import nltk

nltk.download('punkt')

df['num_characters']=df['message'].apply(len)
df.head()

from nltk.tokenize import word_tokenize

df['message'].apply(lambda x: nltk.word_tokenize(x))

df['num_words']=df['message'].apply(lambda x:len(nltk.word_tokenize(x)))
df.sample(5)

df['num_sentences']=df['message'].apply(lambda x: len(nltk.sent_tokenize(x)))
df.sample(5)

df[df['message_type']==0][['num_characters','num_words','num_sentences']].describe()

df[df['message_type']==1][['num_characters','num_words','num_sentences']].describe()

plt.figure(figsize=(12,6))

sns.histplot(df[df['message_type']==0]['num_characters'],color='green')
sns.histplot(df[df['message_type']==1]['num_characters'],color='red')

plt.figure(figsize=(12,6))
sns.histplot(df[df['message_type']==0]['num_words'],color='green')
sns.histplot(df[df['message_type']==1]['num_words'],color='red')

sns.heatmap(df.corr(),annot=True)

def text_transform(txt):
  txt=txt.lower()
  txt=nltk.word_tokenize(txt)

  y=[]
  for i in txt:
    if i.isalnum():
      y.append(i)
    y.clear()
  
  for i in txt:
    if i not in stopwords.words('english') and i not in string.punctuation:
      y.append(i)
  message=y[:]
  y.clear()

  for i in message :
    y.append(ps.stem(i))
  return " ".join(y)

nltk.download('stopwords')
  from nltk.corpus import stopwords
  stopwords.words('english')
  len(stopwords.words('english'))

import string
  string.punctuation

from nltk.stem.porter import PorterStemmer
  ps =PorterStemmer()
  df['transformed_msg']=df['message'].apply(text_transform)
  df.sample(5)

from wordcloud import WordCloud 
wc=WordCloud(width=900,height=900,min_font_size=20,background_color="white")

spam_wc=wc.generate(df[df['message_type']==1]['transformed_msg'].str.cat(sep=""))

plt.figure(figsize=(18,12))
plt.imshow(spam_wc)

ham_wc=wc.generate(df[df['message_type']==0]['transformed_msg'].str.cat(sep=""))

plt.figure(figsize=(18,12))

plt.imshow(ham_wc)

sc=[]
for msg in df[df['message_type']==1]['transformed_msg'].tolist():
  for word in msg.split():
    sc.append(word)

from collections import Counter
Counter(sc)

Counter(sc).most_common(30)

plt.figure(figsize=(18,12))
sns.barplot(x=pd.DataFrame(Counter(sc).most_common(30))[0],y=pd.DataFrame(Counter(sc).most_common(30))[1])
plt.xticks(rotation='vertical')
plt.show()

